# SD Hypertextbook

## Preamble
This is a tutorial/commentary to guide a newcomer how to setup and use Stable Diffusion to its fullest. It's a pruned version of the [SD Goldmine](https://rentry.org/sdgoldmine)

## Contact
If you have questions or comments, contact me

Socials: 
Trip: questianon !!YbTGdICxQOw 
Discord: malt#6065
Reddit: u/questianon
Github: https://github.com/questianon
Twitter: https://twitter.com/questianon)

## How to use this resource
The hypertextbook is ordered from surface-level content to deep level content. If you are a newcomer to Stable Diffusion, it's highly recommended to start from the beginning. If you need a resource that isn't listed here, check out the [Goldmine](https://rentry.org/sdgoldmine)

## Chapters
- [Chapter 1: Getting Started](#getting-started)
	- [Prerequisites](#prerequisites)
	- [Instructions](#instructions)
	- [Hello Asuka](#hello-asuka)
- [Chapter 2: Troubleshooting](#troubleshooting)
- [Chapter 3: The Webui Pt.1](#webui-pt-1)
- [Chapter 4: Prompting](#prompting)
	- [Txt2img](#txt2img)
	- [Img2img](#img2img)
	- [Inpainting](#inpainting)
	- [PNG Info](#png-info)
	- [Extra](#extras)
	- [Extensions](#extensions)
- [Chapter 5: The Webui Pt.2](#webui-pt-2)
	- [Models](#models)
	- [Dreambooth](#dreambooth)
	- [Embeddings](#embeddings)
	- [Hypernetworks](#hypernetworks)
	- [Misc](#misc)
- [Chapter 6: Training](#training)
- [Chapter 7: Developments](#developments)
	- [NAI](#nai)
	- [Anything.ckpt](#anythingckpt)
- [Chapter 8: Modifications](#modifications)
- [Chapter 9: Common Questions](#common-questions)
- [Chapter 10: Resources](#resources)

## Getting Started
Stable Diffusion is easy to setup. It takes ~5 minutes to set up if you have all the files.

I highly personally recommend using the [NAI Speedrun](https://rentry.org/nai-speedrun) if this guide doesn't cover something yet, though any from [Goldmine: Getting Started](https://rentry.org/sdgoldmine#getting-started) should work

### Prerequisites (Windows + NVIDIA)
[**Git**](https://git-scm.com/download/win) 
- Activate option `Windows Explorer integration > Git Bash`
- Everything else is fine, change what you want

[**Python**](https://www.python.org/downloads/windows/) 
- Make sure `add to PATH` is enabled
- Everything else if fine, change what you want

Enough space on your computer (my install, which is pretty barebones, takes up 8 GBs)

A model. For this example, let's use NAI.ckpt (go to [Developments](#developments) for more information on what this is): 

Anonfiles: https://anonfiles.com/U5Acl7F0y2/Novel_AI_Hypernetworks_zip
Pixeldrain: https://pixeldrain.com/u/FMJ4TQbM

### Instructions
1. Open Windows Explorer and navigate to where you want AUTOMATIC1111's webui to be installed. The example directory for this tutorial will be `C:\webui` for simplicity. This is a dummy variable and should be changed to whatever your desired directory is
2. Right click anywhere in your desired directory (e.g. `C:\webui`) and select `Git Bash Here`. If this is missing, you didn't activate the option `Windows Explorer integration > Git Bash` when installing. Either change the option somehow or just reinstall Git
3. Type/paste `git clone https://github.com/AUTOMATIC1111/stable-diffusion-webui` and click enter. Wait for the download to finish
4. Navigate to the stable-diffusion-webui-master\ folder where you should see a file named `webui-user.bat`. For this tutorial's example, this folder will be `C:\webui\stable-diffusion-webui-master\`
5. Navigate to `stable-diffusion-webui-master\models\Stable-diffusion` and place `model.ckpt` and `animevae.pt` in here
	- For now, rename them as follows:
		- `model.ckpt` --> `animefinal-full-pruned.ckpt`
		- `animevae.pt` --> `animefinal-full-pruned.vae.pt`
6. Run `webui-user.bat`

Everything's good to go when `Running on local URL: http://127.0.0.1:7860` is displayed. Open a web browser and type in `http://127.0.0.1:7860` to access your local SD install.

### Hello Asuka

The reason why we use NAI is because of the Asuka Test (similar to the `Hello World` test for programming)

1. Check the top left under the title `Stable Diffusion checkpoint` to see if `animefinal-full-pruned.ckpt` is there. If it's not, click the box and click `animefinal-full-pruned.ckpt`
2. Verify the files loaded correctly via the following messages in the console log:
	- `Loading weights [925997e9] from: [your directory here]\stable-diffusion-webui\models\Stable-diffusion\animefull-final-pruned.ckpt`
	- `Loading VAE weights from: [your directory here]\stable-diffusion-webui\models\Stable-diffusion\animefull-final-pruned.vae.pt`
3. Go to the `Settings` tab at the top
4. `Ctrl + F` and make the following changes:
	- Stop At last layers of CLIP model = 2
	- Eta noise seed delta = 31337
5. Click `Apply settings` at the top of the page and verify that the changes have been saved (it will say `2 settings changed`)
6. Go to the `txt2img` tab at the top
7. (Euler) Use the following in their respective fields:
	- Prompt: `masterpiece, best quality, masterpiece, asuka langley sitting cross legged on a chair`
	- Negative prompt: `lowres, bad anatomy, bad hands, text, error, missing fingers, extra digit, fewer digits, cropped, worst quality, low quality, normal quality, jpeg artifacts,signature, watermark, username, blurry, artist name`
	- Sampling Steps: `28`
	- Sampling Method: `Euler`
	- CFG Scale: `12`
	- Seed: `2870305590`

8. (Euler a, optional) This is if you really want to make sure your setup is correct:
	- Prompt: `masterpiece portrait of smiling Asuka \(evangelion\), evangelion \(Hideaki\), caustics, textile shading, high resolution illustration, blue eyes, contempt, feminine, woman disdain, disgust, no pupils, hair over face,orange hair, long hair, red suit, ilya kuvshinov`
	- Negative prompt: `nsfw, lowres, bad anatomy, bad hands, text, error, missing fingers, extra digit, fewer digits, cropped, worst quality, low quality, normal quality, jpeg artifacts, signature, watermark, username, blurry, headdress, loli,`
	- Sampling Steps: `28`
	- Sampling Method: `Euler a`
	- CFG Scale: `12`
	- Seed: `2870305590`
9. Click `Generate`

The following Asuka's should be generated within 95%-100% correctness:
- Euler a: https://i.imgur.com/j7DSvIS.png
- Euler: https://i.imgur.com/Bfl5qJB.png

If you have issues with your Asuka test, check out one of the following depending on which Asuka test you're recreating:
	- Asuka Euler: https://imgur.com/a/DCYJCSX
	- Asuka Euler a: https://imgur.com/a/s3llTE5

## Troubleshooting

Adding commands to your `commandline args` means to do the following:
1. Right click `webui-user.bat`
2. Click 'Edit'
3. Add your commands
4.  Save
Look at the first issue + solution for an example

- If you're running out of vram (memory), right click `webui-user.bat`, click 'Edit', and add " --med-vram " or " --low-vram " (remove the quotations) after 'set COMMANDLINE_ARGS=' in webui-user.bat. Choose the former if possible
- If you have a 16xx card, add " --precision full --no-half " (remove the quotations) to your commandline args in webui-user.bat
- If your outputs are randomly black, add " --no-half-vae " (remove the quotations) to your commandline args in webui-user.bat

## Webui Pt 1

### txt2img
The bread and butter of Stable Diffusion. You type stuff in and the AI generates stuff for you

- Prompt: Pushes the vectors in the positive direction every step  
- Negative prompt: Pushes the vectors in the negative direction every step  
- Sampling steps: Generally, more steps = more accurate. Exception: the "a samplers" (like `Euler a`) completely change after every ~20 steps.  
- Sampling method: Each one generates images differently. Test it out yourself to see which you like. For comparisons, check out the goldmine and search `comparisons`  
- Width and height: Output dimensions. The larger these are, the more vram is required  
- Restore faces: Restores faces using the `face restoration model` in `Settings`. You can also restore faces in the `Extras` tab if you don't want it to auto restore faces after your generation  
- Tiling: Meant for generations with patterns  
- Highres. fix: Only really useful for generations above 512x512. It breaks down the output resolution into 512x512 chunks, runs txt2img on each one, then combines them together through img2img (I believe). Very helpful in dealing with multiple people in the output image.
	- Firstpass width/height: Will generate the initial chunks at this resolution. Most commonly left on 0x0  
	- Denoising strength: How much the output image will differ from the chunked input images. Play around with this to see what you like. I usually run ~0.3  
- Batch count: How many images to generate in total  
- Batch size: How many images to generate at a time. Speeds up it/s but uses more vram  
- CFG scale: How closely the AI should follow the prompt. Higher values can deepfry your output while lower values can make the AI ignore your prompt  
- Seed: Randomization, basically a Minecraft seed, -1 is to randomize  
- Extra: No one really uses these
	- Variation seed: Seed of a different picture to be mixed into the generation. Basically combines parts of seeds together 
	- Variation strength: How strong of a variation to produce. 0 = original image, 1 = variation image
	- Resize seed from width/height: Attempts to prdouce a picture similar to what would have been produced with the same seed at the specified resolution
- Script: Custom user scripts
- The paint emoji: Adds a random artist to the prompt from the artists.csv file in `\stable-diffusion-webui`
- The checkmark emoji: Read generation parameters from prompt or last generation if prompt is empty into user interface. Seems to take the last generation and put it's prompt + settings into the current UI
- The floppy disk emoji: Saves the current prompt as a style
- The clipboard emojiL Applies the selected styles to the current prompt
- Style 1 and 2: When you save a style, it saves your current prompt. Using a style just adds that prompt secretly (not sure if it's at the end or the beginning)  
- Send buttons: Sends your output to the desired location so you don't need to find it in your output folder and drag it back in  

### img2img
The peanut butter and jam of Stable Diffusiono

You can edit and generate similar images

- Just resize: Will resize output image to target resolution. You need to make your output width x height match your input's wxh
- Crop and resize: Will resize the output image so the entirety of the target resolution is filled with the image. Will crop parts that stick out
- Resize and fill: Will resize the output image so that the entire image is inside the target resoltuion. Will fill empty space with output image's colors
- Width/Height: The resolution of your output image. Make sure it matches your input unless you deliberately want to not make them match
- Denoising strength: How much of the original image should be respected by the algorithm. 0 = nothing changes while 1 = a completely unrelated image. Unless changed in the settings, values below 1 will take less steps than the `Sampling Steps` slider specifies
- Interrogate CLIP: Will try to generate a CLIP prompt out of your input image. For me, there seems to be a memory leak that breaks my Stable Diffusion after using it, so your mileage may vary.

### Inpainting
The butter knife of Stable Diffusion

You can edit/change parts of the image that you don't like

## Resources
News: https://rentry.org/sdupdates, https://rentry.org/sdupdates2, https://rentry.org/sdupdates3
Goldmine (Literally everything else): https://rentry.org/sdgoldmine